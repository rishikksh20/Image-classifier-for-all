{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "# from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "# from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras import applications\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# other imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import pickle as cPickle\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width, img_height = 224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = applications.InceptionV3(weights='imagenet',include_top=False, input_shape = (img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishikesh/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"mi..., inputs=Tensor(\"in...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "inceptionv3=Model(input=model.input, output=model.get_layer('mixed10').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"model_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = inceptionv3.to_json()\n",
    "with open(\"model_json/inceptionv3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    x = base_model.output\\n    x = GlobalAveragePooling2D()(x)\\n    predictions = Dense(nb_classes, activation='softmax')(x)\\n\\n    # add your top layer block to your base model\\n    model = Model(base_model.input, predictions)\\n    print(model.summary())\\n\\n    # # let's visualize layer names and layer indices to see how many layers/blocks to re-train\\n    # # uncomment when choosing based_model_last_block_layer\\n    # for i, layer in enumerate(model.layers):\\n    #     print(i, layer.name)\\n\\n    # first: train only the top layers (which were randomly initialized)\\n    # i.e. freeze all layers of the based model that is already pre-trained.\\n    for layer in base_model.layers:\\n        layer.trainable = False\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### For fine tune\n",
    "# Top Model Block\n",
    "'''\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "    # add your top layer block to your base model\n",
    "    model = Model(base_model.input, predictions)\n",
    "    print(model.summary())\n",
    "\n",
    "    # # let's visualize layer names and layer indices to see how many layers/blocks to re-train\n",
    "    # # uncomment when choosing based_model_last_block_layer\n",
    "    # for i, layer in enumerate(model.layers):\n",
    "    #     print(i, layer.name)\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all layers of the based model that is already pre-trained.\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishikesh/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"bl..., inputs=Tensor(\"in...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model_ = applications.VGG16(weights='imagenet',include_top=False, input_shape = (img_width, img_height, 3))\n",
    "vgg=Model(input=model_.input, output=model_.get_layer('block5_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = vgg.to_json()\n",
    "with open(\"model_json/vgg16.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishikesh/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"bl..., inputs=Tensor(\"in...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "xception =applications.Xception(weights='imagenet',include_top=False, input_shape = (img_width, img_height, 3))\n",
    "feat_ext_xception=Model(input=xception.input, output=xception.get_layer(\"block14_sepconv2_act\").output)\n",
    "# serialize model to JSON\n",
    "model_json = feat_ext_xception.to_json()\n",
    "with open(\"model_json/xception.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## After save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# other imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import pickle as cPickle\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width, img_height = 224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = \"data/MSTAR_Chips/TRAIN_images\"\n",
    "validation_data_dir = \"data/MSTAR_Chips/TEST_images\"\n",
    "nb_train_samples = 1622\n",
    "nb_validation_samples = 1365 \n",
    "batch_size = 32\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to training dataset\n",
    "train_labels = os.listdir(train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json file path : model_json/inceptionv3.json\n",
      "Weight path : model_json/inceptionv3.h5\n",
      "Json file path : model_json/vgg16.json\n",
      "Weight path : model_json/vgg16.h5\n",
      "Json file path : model_json/xception.json\n",
      "Weight path : model_json/xception.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "models=[]\n",
    "model_names=[]\n",
    "json_dir_name = \"model_json\"\n",
    "weight_name=\"\"\n",
    "json_pattern = os.path.join(json_dir_name,'*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "for file in file_list:\n",
    "    print(\"Json file path :\",file)\n",
    "    # load json and create model\n",
    "    json_file = open(file, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    weight_name= file.split(\".\")\n",
    "    print(\"Weight path : {}.h5\".format(weight_name[0]))\n",
    "    #load weights into new model\n",
    "    loaded_model.load_weights(\"{}.h5\".format(weight_name[0]))\n",
    "    models.append(loaded_model)  \n",
    "    model_names.append(weight_name[0].split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_names=[\"inceptionv3\",\"vgg16\",\"xception\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/MSTAR_Chips/TRAIN_images/bmp2_tank\n",
      "data/MSTAR_Chips/TRAIN_images/btr70_transport\n",
      "data/MSTAR_Chips/TRAIN_images/t72_tank\n"
     ]
    }
   ],
   "source": [
    "image_size=(224,224)\n",
    "h5f.close()\n",
    "for model,name in zip(models,model_names):\n",
    "    # variables to hold features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    # h5f = h5py.File('features/{}.h5'.format(name), 'w')\n",
    "    with h5py.File('features/{}.hdf5'.format(name),'w') as f:\n",
    "        # loop over all the labels in the folder\n",
    "        for label in train_labels:\n",
    "            cur_path = train_data_dir + \"/\" + label\n",
    "            print(cur_path)\n",
    "            for image_path in glob.glob(cur_path + \"/*.jpg\"):\n",
    "                img = image.load_img(image_path, target_size=image_size)\n",
    "                x = image.img_to_array(img)\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                x = preprocess_input(x)\n",
    "                feature = model.predict(x)\n",
    "                flat = feature.flatten()\n",
    "                features.append(flat)\n",
    "                labels.append(label)\n",
    "        f.create_dataset('train', data=features)\n",
    "        f.create_dataset('train_label', data=labels)\n",
    "        # loop over all the labels in the folder\n",
    "        for label in train_labels:\n",
    "            cur_path = validation_data_dir + \"/\" + label\n",
    "            for image_path in glob.glob(cur_path + \"/*.jpg\"):\n",
    "                img = image.load_img(image_path, target_size=image_size)\n",
    "                x = image.img_to_array(img)\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                x = preprocess_input(x)\n",
    "                feature = feat_ext_vgg.predict(x)\n",
    "                flat = feature.flatten()\n",
    "                test_features.append(flat)\n",
    "                test_labels.append(label)\n",
    "        f.create_dataset('test', data=test_features)\n",
    "        f.create_dataset('test_label', data=test_labels)\n",
    "    #h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
