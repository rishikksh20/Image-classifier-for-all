{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "# from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "# from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras import applications\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# other imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import pickle as cPickle\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width, img_height = 224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = applications.InceptionV3(weights='imagenet',include_top=False, input_shape = (img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishikesh/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"mi..., inputs=Tensor(\"in...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "inceptionv3=Model(input=model.input, output=model.get_layer('mixed10').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"model_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = inceptionv3.to_json()\n",
    "with open(\"model_json/inceptionv3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    x = base_model.output\\n    x = GlobalAveragePooling2D()(x)\\n    predictions = Dense(nb_classes, activation='softmax')(x)\\n\\n    # add your top layer block to your base model\\n    model = Model(base_model.input, predictions)\\n    print(model.summary())\\n\\n    # # let's visualize layer names and layer indices to see how many layers/blocks to re-train\\n    # # uncomment when choosing based_model_last_block_layer\\n    # for i, layer in enumerate(model.layers):\\n    #     print(i, layer.name)\\n\\n    # first: train only the top layers (which were randomly initialized)\\n    # i.e. freeze all layers of the based model that is already pre-trained.\\n    for layer in base_model.layers:\\n        layer.trainable = False\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### For fine tune\n",
    "# Top Model Block\n",
    "'''\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "    # add your top layer block to your base model\n",
    "    model = Model(base_model.input, predictions)\n",
    "    print(model.summary())\n",
    "\n",
    "    # # let's visualize layer names and layer indices to see how many layers/blocks to re-train\n",
    "    # # uncomment when choosing based_model_last_block_layer\n",
    "    # for i, layer in enumerate(model.layers):\n",
    "    #     print(i, layer.name)\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all layers of the based model that is already pre-trained.\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishikesh/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"bl..., inputs=Tensor(\"in...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model_ = applications.VGG16(weights='imagenet',include_top=False, input_shape = (img_width, img_height, 3))\n",
    "vgg=Model(input=model_.input, output=model_.get_layer('block5_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = vgg.to_json()\n",
    "with open(\"model_json/vgg16.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishikesh/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"bl..., inputs=Tensor(\"in...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "xception =applications.Xception(weights='imagenet',include_top=False, input_shape = (img_width, img_height, 3))\n",
    "feat_ext_xception=Model(input=xception.input, output=xception.get_layer(\"block14_sepconv2_act\").output)\n",
    "# serialize model to JSON\n",
    "model_json = feat_ext_xception.to_json()\n",
    "with open(\"model_json/xception.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## After save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# other imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import pickle as cPickle\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width, img_height = 224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = \"data/MSTAR_Chips/TRAIN_images\"\n",
    "validation_data_dir = \"data/MSTAR_Chips/TEST_images\"\n",
    "nb_train_samples = 1622\n",
    "nb_validation_samples = 1365 \n",
    "batch_size = 32\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to training dataset\n",
    "train_labels = os.listdir(train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json file path : model_json\\inceptionv3.json\n",
      "Weight path : model_json\\inceptionv3.h5\n",
      "Json file path : model_json\\vgg16.json\n",
      "Weight path : model_json\\vgg16.h5\n",
      "Json file path : model_json\\xception.json\n",
      "Weight path : model_json\\xception.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "models=[]\n",
    "model_names=[]\n",
    "json_dir_name = \"model_json\"\n",
    "weight_name=\"\"\n",
    "json_pattern = os.path.join(json_dir_name,'*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "for file in file_list:\n",
    "    print(\"Json file path :\",file)\n",
    "    # load json and create model\n",
    "    json_file = open(file, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    weight_name= file.split(\".\")\n",
    "    print(\"Weight path : {}.h5\".format(weight_name[0]))\n",
    "    #load weights into new model\n",
    "    loaded_model.load_weights(\"{}.h5\".format(weight_name[0]))\n",
    "    models.append(loaded_model)  \n",
    "    model_names.append(weight_name[0].split(\"\\\\\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inceptionv3', 'vgg16', 'xception']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/MSTAR_Chips/TRAIN_images/bmp2_tank\n",
      "data/MSTAR_Chips/TRAIN_images/btr70_transport\n",
      "data/MSTAR_Chips/TRAIN_images/t72_tank\n",
      "data/MSTAR_Chips/TRAIN_images/bmp2_tank\n",
      "data/MSTAR_Chips/TRAIN_images/btr70_transport\n",
      "data/MSTAR_Chips/TRAIN_images/t72_tank\n",
      "data/MSTAR_Chips/TRAIN_images/bmp2_tank\n",
      "data/MSTAR_Chips/TRAIN_images/btr70_transport\n",
      "data/MSTAR_Chips/TRAIN_images/t72_tank\n"
     ]
    }
   ],
   "source": [
    "image_size=(224,224)\n",
    "\n",
    "with open(\"label.pkl\", \"wb\") as label_file:\n",
    "    for model,name in zip(models,model_names):\n",
    "        features = []\n",
    "        labels = []\n",
    "        # h5f = h5py.File('features/{}.h5'.format(name), 'w')\n",
    "        with h5py.File('features/{}.hdf5'.format(name),'w') as f:\n",
    "            # loop over all the labels in the folder\n",
    "            for label in train_labels:\n",
    "                cur_path = train_data_dir + \"/\" + label\n",
    "                print(cur_path)\n",
    "                for image_path in glob.glob(cur_path + \"/*.jpg\"):\n",
    "                    img = image.load_img(image_path, target_size=image_size)\n",
    "                    x = image.img_to_array(img)\n",
    "                    x = np.expand_dims(x, axis=0)\n",
    "                    x = preprocess_input(x)\n",
    "                    feature = model.predict(x)\n",
    "                    flat = feature.flatten()\n",
    "                    features.append(flat)\n",
    "                    labels.append(label)\n",
    "            f.create_dataset('train', data=features)\n",
    "    cPickle.dump(labels, label_file)\n",
    "\n",
    "with open(\"test_label.pkl\", \"wb\") as label_file:\n",
    "    for model,name in zip(models,model_names):\n",
    "        # h5f = h5py.File('features/{}.h5'.format(name), 'w')\n",
    "        with h5py.File('features/{}.h5'.format(name),'w') as f:\n",
    "            test_features = []\n",
    "            test_labels = []\n",
    "            # loop over all the labels in the folder\n",
    "            for label in train_labels:\n",
    "                cur_path = validation_data_dir + \"/\" + label\n",
    "                for image_path in glob.glob(cur_path + \"/*.jpg\"):\n",
    "                    img = image.load_img(image_path, target_size=image_size)\n",
    "                    x = image.img_to_array(img)\n",
    "                    x = np.expand_dims(x, axis=0)\n",
    "                    x = preprocess_input(x) \n",
    "                    feature = model.predict(x)\n",
    "                    flat = feature.flatten()\n",
    "                    test_features.append(flat)\n",
    "                    test_labels.append(label)\n",
    "            f.create_dataset('test', data=test_features)\n",
    "            \n",
    "    cPickle.dump(test_labels, label_file)\n",
    "#h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For prediction work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('test_label.pkl', 'rb') as f:\n",
    "    test_labels = cPickle.load(f)\n",
    "    \n",
    "with open('label.pkl', 'rb') as f:\n",
    "    labels = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode the labels using LabelEncoder\n",
    "targetNames = np.unique(labels)\n",
    "le = LabelEncoder()\n",
    "le_labels = le.fit_transform(labels)\n",
    "test_labels=le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json file path : features\\inceptionv3.hdf5\n",
      "Json file path : features\\vgg16.hdf5\n",
      "Json file path : features\\xception.hdf5\n"
     ]
    }
   ],
   "source": [
    "feat_pattern = os.path.join(\"features/\",'*.hdf5')\n",
    "file_list = glob.glob(feat_pattern)\n",
    "feat=[]\n",
    "for file in file_list:\n",
    "    print(\"Json file path :\",file)\n",
    "    # load json and create model\n",
    "    with  h5py.File(file, 'r') as f:\n",
    "        feat.append(f['train'][:])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feat:  (1622, 51200)\n",
      "Features:  (1622, 51200)\n",
      "Feat:  (1622, 25088)\n",
      "Features:  (1622, 76288)\n",
      "Feat:  (1622, 100352)\n",
      "Features:  (1622, 176640)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for feature in feat:\n",
    "    if i==0:\n",
    "        features=np.asarray(feature)\n",
    "    else:\n",
    "        features = np.concatenate( [features,np.asarray(feature)], axis=1 )\n",
    "    print(\"Feat: \",np.asarray(feature).shape)\n",
    "    print(\"Features: \",features.shape)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json file path : features\\inceptionv3.h5\n",
      "Json file path : features\\vgg16.h5\n",
      "Json file path : features\\xception.h5\n",
      "Feat:  (1365, 51200)\n",
      "Features:  (1365, 51200)\n",
      "Feat:  (1365, 25088)\n",
      "Features:  (1365, 76288)\n",
      "Feat:  (1365, 100352)\n",
      "Features:  (1365, 176640)\n"
     ]
    }
   ],
   "source": [
    "test_feat_pattern = os.path.join(\"features/\",'*.h5')\n",
    "file_list = glob.glob(test_feat_pattern)\n",
    "test_feat=[]\n",
    "for file in file_list:\n",
    "    print(\"Json file path :\",file)\n",
    "    # load json and create model\n",
    "    with  h5py.File(file, 'r') as f:\n",
    "        test_feat.append(f['test'][:]) \n",
    "        \n",
    "i=0\n",
    "for feature in test_feat:\n",
    "    if i==0:\n",
    "        test_features=np.asarray(feature)\n",
    "    else:\n",
    "        test_features = np.concatenate( [test_features,np.asarray(feature)], axis=1 )\n",
    "    print(\"Feat: \",np.asarray(feature).shape)\n",
    "    print(\"Features: \",test_features.shape)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# organize imports\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] creating model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=2017, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use logistic regression as the model\n",
    "print(\"[INFO] creating model...\")\n",
    "model = LogisticRegression(random_state=seed)\n",
    "model.fit(features, le_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      bmp2_tank       0.87      0.92      0.90       555\n",
      "btr70_transport       0.79      0.85      0.82       181\n",
      "       t72_tank       0.97      0.90      0.94       629\n",
      "\n",
      "    avg / total       0.91      0.90      0.91      1365\n",
      "\n",
      "Accuracy : 0.904029304029\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model of test data\n",
    "preds = model.predict(test_features)\n",
    "print(classification_report(le.inverse_transform(preds),le.inverse_transform(test_labels)))\n",
    "print(\"Accuracy :\",accuracy_score(le.inverse_transform(preds),le.inverse_transform(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
